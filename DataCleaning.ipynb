{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data acquisition and cleaning\n",
    "**This part is only dedicated to how the data was acquired and cleaned**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two sources of data:\n",
    "- Provided data from ICC\n",
    "- Crawled data from web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process of the provided data\n",
    "The provided data was easily parsed with the `Perl` script given by the TA, no cleaning was necessary here, removing NaN values and dropping duplicates is done when loading the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process of the crawled data\n",
    "We used several own-made `BASH` script to fetch and retrieve data from a given website.\n",
    "\n",
    "The first was made by hand (the structure of the folders was done with `wget -x`): we retrieve each category of regional cuisines and create separated folders. In each folder there was the **index.html** page of the corresponding regional cuisine page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, by using the following script, we retrieve links for each category we had previously found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-e390fcf6d71f>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-e390fcf6d71f>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    STARTING=$PWD\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# fetcher.sh\n",
    "#!/usr/bin/env bash\n",
    "STARTING=$PWD\n",
    "for directory in $(find $STARTING -type d); \n",
    "do\n",
    "    cd \"$directory\"\n",
    "    url=$(cat *.html* | grep \"canonical*\"  | sed \"s/.*href=\\\"//\" | sed \"s/\\\" \\/>/?page=/\")\n",
    "\n",
    "    $STARTING/./crawling.sh $url 2\n",
    "    sleep 5\n",
    "    cd $STARTING\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawling.sh\n",
    "#!/usr/bin/env bash\n",
    "for i in $(eval echo {1..$2})\n",
    "do\n",
    " TARGET=\"${1/$'\\r'/}$i\"\n",
    "    --wait=10 \\\n",
    "    --random-wait \\\n",
    "    --reject '*.js,*.css,*.ico,*.gif,*.jpg,*.jpeg,*.png,*.mp3,*.pdf,*.tgz,*.flv,*.avi,*.mpeg,*.iso' \\\n",
    "    --execute robots=off \\\n",
    "    --user-agent=AGENT \\\n",
    "    --convert-links \\\n",
    "    --no-cache \\\n",
    "    --no-clobber \\\n",
    "    --no-http-keep-alive \\\n",
    "    --follow-tags=a/href \\\n",
    "    --accept=html \\\n",
    "    --header=\"Accept: text/html\" \\\n",
    "    --ignore-tags=img,link,script \\\n",
    "    $TARGET\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this first step, we had a *urls.txt* file for each subfolder, which has all the recipes link for a given category.  \n",
    "Last step was to execute for each line the following script.  \n",
    "It downloads the page into a temporary `HTML` file, retrieves the required data and timeouts for 5 seconds to avoid the website robot to detect us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env bash\n",
    "\n",
    "STARTING=$PWD\n",
    "TMP_FILE=\"tmp.html\"\n",
    "DATA_FILE=\"data.csv\"\n",
    "DESC_FILE=\"desc.csv\"\n",
    "URL_LISTS=\"urls.txt\"\n",
    "\n",
    "for directory in $(find $STARTING -type d); \n",
    "do\n",
    "    cd \"$directory\"\n",
    "    for url in $(cat $URL_LISTS)\n",
    "    do\n",
    "        #################################### Downloading\n",
    "        wget \\\n",
    "            --wait=10 \\\n",
    "            --random-wait \\\n",
    "            --reject '*.js,*.css,*.ico,*.gif,*.jpg,*.jpeg,*.png,*.mp3,*.pdf,*.tgz,*.flv,*.avi,*.mpeg,*.iso' \\\n",
    "            --execute robots=off \\\n",
    "            --user-agent=AGENT \\\n",
    "            --convert-links \\\n",
    "            --no-cache \\\n",
    "            --no-clobber \\\n",
    "            --no-http-keep-alive \\\n",
    "            --output-document=\"$TMP_FILE\" \\\n",
    "            \"$url\"\n",
    "\n",
    "        #################################### Parsing\n",
    "        # Main info -> inggredients\n",
    "        hash=$(md5sum $TMP_FILE | sed \"s/  $TMP_FILE.*//\")\n",
    "        title=$(cat $TMP_FILE | grep \"<title>\" | sed \"s/.*<title>//\" | sed \"s/Recipe - Allrecipes.*//\")\n",
    "        ing=$(cat $TMP_FILE | grep \"checkList__item'\\}\\[true\\]\" | sed \"s/.*title=\\\"//\" | sed \"s/\\\">//\" | tr \"\\r\" \" \" | tr \"\\n\" \"|\")\n",
    "\n",
    "        # Nutritive\n",
    "        nutritive=$(cat $TMP_FILE | grep -A 20 \"<div class=\\\"nutrition-summary-facts\\\">\" | grep \"itemprop\")\n",
    "\n",
    "        # Calories values\n",
    "        cal=$(echo \"$nutritive\" | grep \"calorie*\" | sed 's/<span itemprop=\\\"calories\\\">//' | sed \"s/ calories;<\\/span.*//\" | sed 's/[[:blank:]]//g' | sed ':a;N;$!ba;s/\\n//g')\n",
    "\n",
    "        # Fat values\n",
    "        fat=$(echo \"$nutritive\" |grep \"fat*\")\n",
    "        val=$(echo \"$fat\" | sed 's/<span itemprop=\\\"fatContent\\\">//' | sed \"s/<span.*//\" | sed 's/[[:blank:]]//g' | sed ':a;N;$!ba;s/\\n//g')\n",
    "        if [[ $val ]]\n",
    "        then\n",
    "            if [[ $(echo \"$fat\" | sed \"s/.*hidden=\\\"true\\\">//\" | grep \"mg\") ]]\n",
    "            then\n",
    "                fat=$val\n",
    "            else\n",
    "                fat=$(echo $val*1000 | bc)\n",
    "            fi\n",
    "        fi\n",
    "\n",
    "        # Carbon values\n",
    "        carb=$(echo \"$nutritive\" |grep \"carbon*\")\n",
    "        val=$(echo \"$carb\" | sed 's/<span itemprop=\\\"carbohydrateContent\\\">//' | sed \"s/<span.*//\" | sed 's/[[:blank:]]//g' | sed ':a;N;$!ba;s/\\n//g')\n",
    "        if [[ $val ]]\n",
    "        then\n",
    "            if [[ $(echo \"$carb\" | sed \"s/.*hidden=\\\"true\\\">//\" | grep \"mg\") ]]\n",
    "            then\n",
    "                carb=$val\n",
    "            else\n",
    "                carb=$(echo $val*1000 | bc)\n",
    "            fi\n",
    "        fi\n",
    "\n",
    "        # Protein values\n",
    "        prot=$(echo \"$nutritive\" |grep \"prot*\")\n",
    "        val=$(echo \"$prot\" | sed 's/<span itemprop=\\\"proteinContent\\\">//' | sed \"s/<span.*//\" | sed 's/[[:blank:]]//g' | sed ':a;N;$!ba;s/\\n//g')\n",
    "        if [[ $val ]]\n",
    "        then\n",
    "            if [[ $(echo \"$prot\" | sed \"s/.*hidden=\\\"true\\\">//\" | grep \"mg\") ]]\n",
    "            then\n",
    "                prot=$val\n",
    "            else\n",
    "                prot=$(echo $val*1000 | bc)\n",
    "            fi\n",
    "        fi\n",
    "\n",
    "        # Cholesterol values\n",
    "        chol=$(echo \"$nutritive\" |grep \"chol*\")\n",
    "        val=$(echo \"$chol\" | sed 's/<span itemprop=\\\"cholesterolContent\\\">//' | sed \"s/<span.*//\" | sed 's/[[:blank:]]//g' | sed ':a;N;$!ba;s/\\n//g')\n",
    "        if [[ $val ]]\n",
    "        then\n",
    "            if [[ $(echo \"$chol\" | sed \"s/.*hidden=\\\"true\\\">//\" | grep \"mg\") ]]\n",
    "            then\n",
    "                chol=$val\n",
    "            else\n",
    "                chol=$(echo $val*1000 | bc)\n",
    "            fi\n",
    "        fi\n",
    "\n",
    "        # Sodium values\n",
    "        sod=$(echo \"$nutritive\" |grep \"sodium*\")\n",
    "        val=$(echo \"$sod\" | sed 's/<span itemprop=\\\"sodiumContent\\\">//' | sed \"s/<span.*//\" | sed 's/[[:blank:]]//g' | sed ':a;N;$!ba;s/\\n//g')\n",
    "        if [[ $val ]]\n",
    "        then\n",
    "            if [[ $(echo \"$sod\" | sed \"s/.*hidden=\\\"true\\\">//\" | grep \"mg\") ]]\n",
    "            then\n",
    "                sod=$val\n",
    "            else\n",
    "                sod=$(echo $val*1000 | bc)\n",
    "            fi\n",
    "        fi\n",
    "        ######################################### Get Directives\n",
    "        reg=\"<span class=\\\"recipe-directions__list--item\\\">\"\n",
    "        desc=$(cat \"$TMP_FILE\" | grep \"$reg\" | sed \"s/$reg//\" | tr \"\\n\" \" \" | tr -s \" \")\n",
    "        ######################################### Printout\n",
    "        echo -e \"$hash\\t${PWD##*/}\\t$title\\t$ing\\t$cal\\t$carb\\t$fat\\t$prot\\t$sod\\t$chol\" >> \"$DATA_FILE\"\n",
    "        echo -e \"$hashÂ£$desc\" >> $DESC_FILE\n",
    "        #################################### napping\n",
    "        sleep 5\n",
    "    ######################################### end for URLS\n",
    "    done \n",
    "    ######################################### \n",
    "    cd $STARTING\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: we have also retrieved the textual description to make text analysis on it (e.g time of cooking etc..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import re\n",
    "import os.path\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "DATA_FOLDER = './data/'\n",
    "recipes_df = pd.read_csv(DATA_FOLDER + 'recipes_df.csv', sep='\\t', encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ingredient Parsing\n",
    "**This is just a presentation of how we parsed and clean the ingredient list, the following code is working but it needs some tweaking to make in work in this notebook. To see the result, please read** `DataAnalysis.ipynb` **section 2**\n",
    "\n",
    "In this part we are trying to get a list of ingredients for each recipe. This list should be clean, which means it should contain only the names of the ingredients and no other informations, like quantities.\n",
    "\n",
    "To do this, first we cleaned the list of ingredients by applying a low-case and by removing a set of words chosen manually (contained in `black_list`), then we used the natural language processing library `nltk` to remove words different from nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy for test\n",
    "recipes_copy = recipes_df.copy()\n",
    "\n",
    "# lowercase to be insensitive\n",
    "recipes_copy['Ingredients'] = recipes_copy['Ingredients'].str.lower()\n",
    "\n",
    "# Coerce filtering, removing any occurence of these words as a first filter\n",
    "black_list = ['inches','inch','medium','pounds','pound','ounces','ounces','fluid','ground','tablespoons','tablespoon','cups','cup','teaspoons','teaspoon', 'all-purpose', '\\(.*\\)']\n",
    "recipes_copy['Ingredients'] = recipes_copy['Ingredients'].replace(black_list, '', regex=True)\n",
    "\n",
    "# Remove non alphabetic values expect of '|' which is the seperating char\n",
    "recipes_copy['Ingredients'] = recipes_copy['Ingredients'].str.replace('[^a-zA-Z  -]+', ' ')\n",
    "\n",
    "# Retrieve list of ingredients in overall\n",
    "keywords_list = recipes_copy['Ingredients'].str.split(\" \", expand=True).stack().unique()\n",
    "len(keywords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieve bad ingredients\n",
    "\n",
    "# NLP-related imports\n",
    "import nltk\n",
    "nltk.download('punkt');\n",
    "nltk.download('averaged_perceptron_tagger');\n",
    "\n",
    "# NLP to identify only verbs\n",
    "tokens = nltk.word_tokenize(' '.join(keywords_list))\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "# Fetching the list of non correct word\n",
    "gray_list = [word for word,pos in tagged if not(pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]\n",
    "\n",
    "# Further filtering by removing gray_listed word with regex\n",
    "ingredient_serie = recipes_copy['Ingredients'].replace(gray_list, '')\n",
    "\n",
    "# Retrieve list of ingredients in overall\n",
    "keywords_list = ingredient_serie.str.split(\" \", expand=True).stack().unique()\n",
    "\n",
    "# NLP to identify only nouns\n",
    "tokens = nltk.word_tokenize(' '.join(keywords_list))\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "nouns = [word for word,pos in tagged if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS')]\n",
    "\n",
    "# We need to remove word that a smaller than 3 letters, as we suppose they are not ingredients\n",
    "ing_list = [item for item in nouns if len(item) > 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have a list of ingredients contained in `ing_list`, which can be used to filter our dataset. Unfortunately, as we are going to see below, some ingredients are not spelled correctly while others are not ingredients at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take original ingredients list and split each word to count recurrencies\n",
    "ing_ds = recipes_copy['Ingredients'].str.split(\" \", expand=True) \\\n",
    "                                        .stack().value_counts()  \\\n",
    "                                        .to_frame(name='count')  \\\n",
    "                                        .reset_index()\n",
    "\n",
    "# Keeping only the ingredient in the previous list\n",
    "ing_ds = ing_ds[ing_ds['index'].isin(ing_list)].reset_index(drop=True)\n",
    "\n",
    "#ing_ds.sort_values(by='index') # if you want to see similar words\n",
    "ing_ds.head(21)\n",
    "#ing_ds['index'].to_csv('ing_list') # used to dataclean by hand\n",
    "\n",
    "ing_ds[ing_ds['index'] == 'pioneer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the words `powder`, `taste` and `sauce` are contained in the ten most recurring words, although they are not ingredients. These words should then be parsed by hand and removed to obtain a list that is ingredients-only.\n",
    "\n",
    "We can also notice that some ingredients are duplicated due to different spellings (i.e. `onion` and `onions`). \n",
    "\n",
    "We tried to implement a way to merge similar words by finding a metric that calculates the distance between words. As such the similar words should have close values given by the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieve similar names\n",
    "\n",
    "# We can create a space with N dimensions\n",
    "# Each letter of a word is mapped to its corresponding integer in this space\n",
    "# Similar words will lie closely in this space\n",
    "\n",
    "# Convert ingredient's distribution to list  \n",
    "ing_ds_list = ing_ds['index'].values.tolist()\n",
    "# print(\"\\033[1mbar before sort:\\033[0m\", ing_ds_list)\n",
    "\n",
    "# Looking for the longest word/ingredient\n",
    "N = len(sorted(ing_ds_list, key=len)[-1])\n",
    "# print(\"\\033[1m\\nLongest word is:\\033[0m\", N, \" long\")\n",
    "\n",
    "# For each word in the list, we append the NULL element ASCII to have the same number of elements\n",
    "converted_ing_list = [item + chr(0) * (N - len(item)) for item in ing_ds_list]\n",
    "# print(\"\\033[1m\\n converted_ing_list after padding:\\033[0m\\n\", converted_ing_list)\n",
    "\n",
    "# Convert into each spatiales ASCII -> Numpy matrix\n",
    "word_matrix = np.array([[ord(char) for char in string] for string in converted_ing_list])\n",
    "# print(\"\\033[1m\\n converted_ing_list after ASCII int conversion:\\033[0m\\n\", word_matrix)\n",
    "\n",
    "# Compute the distance between each row \n",
    "# Idea: use backwards propagation to calculate the optimal weights\n",
    "w = [10, 4, 3, 2, 1, 1, 1, 1, 1 ,1 ,1 ,1 ,1 , 1, 1]\n",
    "#distance_matrix = sp.spatial.distance.cdist(word_matrix, word_matrix, 'wminkowski', p=2, w=w)\n",
    "distance_matrix = sp.spatial.distance.cdist(word_matrix, word_matrix, 'euclidean')\n",
    "\n",
    "# print(\"\\033[1m\\nDistance of the matrix define by converted_ing_list:\\033[0m\\n\", distance_matrix)\n",
    "\n",
    "# Thresholding <-> if the distance is small enough words are the same!\n",
    "normed_dist = (distance_matrix < 60).astype(int)\n",
    "# print(\"\\033[1m\\nDistance of the matrix thresholded:\\033[0m\\n\", normed_dist)\n",
    "\n",
    "# The list has been sorted\n",
    "# if we take the first non-zero value for each row we get the matching word\n",
    "vec = normed_dist.argmax(axis=0)\n",
    "# print(\"\\033[1m\\nIndex of corresponding words in sorted [converted_ing_list]:\\033[0m\\n\", vec)\n",
    "\n",
    "# Foo after TODO name\n",
    "deconverted_ing_list = [converted_ing_list[i].replace(chr(0), '') for i in vec]\n",
    "# print(\"\\033[1m deconverted_ing_list operation:\\033[0m\", deconverted_ing_list)\n",
    "\n",
    "# Result\n",
    "ing_dict = dict(zip(ing_ds_list, deconverted_ing_list))\n",
    "\n",
    "ing_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this method is not accurate for now. We would need more time to optimize the weights to use and to filter non-ingredient words.\n",
    "\n",
    "As an alternative option, we envision to clean the ingredient list by hand.\n",
    "\n",
    "This algorithm does not take into account the statistical relevance of letters in the english language, but only alphabetical closeness. \n",
    "\n",
    "**Laste minute update:** We actually can use two different strategies here:\n",
    "- Check if the word exists in the English dictionnary, if there is a word which exists also but only one letter is changing then we combine them (i.e. `onion` and `onions`).\n",
    "- We can implement the [Levenshtein distance](https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Levenshtein_distance) and apply it for each word in the list sorted alphabetically with a of moving window (thus we avoid useless computing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1)\n",
    "\n",
    "    # len(s1) >= len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "\n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
    "            deletions = current_row[j] + 1       # than s2\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    \n",
    "    return previous_row[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this point we clean the data by hand**: the only issue we have now is the similar words which can be fixed by the suggested methods above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_FOLDER + 'cleaned_list') as f:\n",
    "    hand_ing_list = f.read().splitlines()\n",
    "    \n",
    "levenshtein('glaa', 'glaaaaaaaaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gla = [[levenshtein(item1, item2) for item1 in keywords_list] for item2 in keywords_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorry I didn't comment yet but we are doing exactly the same thing as above but with the Levensthein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.array(gla)\n",
    "normed_mat = (matrix <= 1).astype(int)\n",
    "normed_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = normed_mat.argmax(axis=0)\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foo after TODO name\n",
    "transformed_list = [keywords_list[i] for i in vec]\n",
    "# print(\"\\033[1m deconverted_ing_list operation:\\033[0m\", deconverted_ing_list)\n",
    "\n",
    "# Result\n",
    "ing_dict = dict(zip(keywords_list, transformed_list))\n",
    "ing_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving\n",
    "np.save(DATA_FOLDER + 'full_ing_dict.npy', ing_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
